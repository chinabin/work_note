# 0x00. 导读

# 0x01. 简介

MoE = Mixture-of-Experts

# 0x02.

专家混合模型是一种创新的神经网络架构设计，它在 Transformer 架构中融合了众多的专家/模型层。在这种设计中，数据流动时，每一个输入的 Token 都会被动态分配给一些专家进行处理。这种做法使得计算更高效，因为每个专家都能在其擅长的特定任务上发挥出色。
- 专家：MoE 层由众多专家组成，既可以是小型的多层感知机（MLP），也可以是像 Mistral 7B 这样复杂的大语言模型（LLM）。
- 路由器：负责将输入的 Token 分配给合适的专家。路由策略有两种：
  - 由 Token 选择路由器
  - 或由路由器选择 Token。
  - 具体是怎样实现的呢？系统通过一个 softmax 门控函数来建立一个概率分布，从而在众多专家或 Token 中选出最合适的几个。

# 0x03.

混合专家模型 (MoE) 让模型以远低于传统密集模型的计算成本进行预训练，这意味着你可以在相同的计算预算下显著扩大模型或数据集的规模。特别是在预训练阶段，MoE 模型能比其同等规模的密集型模型更快地达到相同的性能水平。

那么，MoE 究竟是什么呢？在 Transformer 模型的背景下，MoE 主要由两个部分组成：

- 稀疏 MoE 层 代替了传统的密集前馈网络 (FFN) 层。MoE 层包含若干“专家”（如 8 个），每个专家都是一个独立的神经网络。实际上，这些专家通常是 FFN，但它们也可以是更复杂的网络，甚至可以是 MoE 本身，形成一个层级结构的 MoE。

- 一个门控网络或路由器，用于决定哪些 Token 分配给哪个专家。值得注意的是，一个 Token 可以被分配给多个专家。如何高效地将 Token 分配给合适的专家，是使用 MoE 技术时需要考虑的关键问题之一。这个路由器由一系列可学习的参数构成，它与模型的其他部分一起进行预训练。

MoE（混合专家模型）的设计思路是这样的：在 Transformer 模型中，我们将每一个 FFN（前馈网络）层替换为 MoE 层，由一个门控网络和若干“专家”组成。

稀疏性基于条件计算的概念。不同于密集模型中所有参数对所有输入都有效，稀疏性让我们能够只激活系统的部分区域。