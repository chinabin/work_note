# 0x00. 导读

# 0x01. 简介

# 0x02.

AI, Artificial Intelligence: 人工智能

GPT(Generative Pre-trained Transformer): 是一种基于 Transformer 架构生成式预训练模型。

token: 一个中文文字对应一个 token，比如冰激凌，对应三个token：冰+激+凌；一个英文的字符对应一个 token，比如 ice cream，对应两个token：ice+cream。

175B、60B、540B等: 这些一般指参数的个数，B 是 Billion/十亿 的意思，175B 是 1750亿参数，这是ChatGPT大约的参数规模。

强化学习:（Reinforcement Learning）一种机器学习的方法，通过从外部获得激励来校正学习方向从而获得一种自适应的学习能力。

LLM(Large Language Model，大语言模型)

涌现，极为宽泛的定义，认为它是一种你未曾明确计划的出乎意料的能力或特性。这个定义的问题在于其极度主观性：不同的人可能会根据他们的期望，在同一个系统中看到或看不到涌现现象。要严肃探讨涌现，我们需要一个更客观的定义。我在此提出一个稍微改进的定义，虽然它也不是完美无缺：如果一个系统显示出的某项属性或能力，在该系统的任何单一组件中都不存在，则这种属性或能力可以被称为 涌现。

# 0x0

符号主义
联结主义
Transformer
LLM