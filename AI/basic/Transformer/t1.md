# 0x00. 导读

# 0x01. 简介

# 0x02.

Embedding: 单词变为张量

Transformer 中单词的输入表示 X 是由 单词Embedding 和 位置Embedding （Positional Encoding）相加得到，通常定义为 Transformer Embedding 层，其实现逻辑如下:
- 单词 Embedding
    - 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。
- 位置 Embedding
    - Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。
    - 位置 Embedding 用 PE 表示，PE 的维度与 单词Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者。

![Alt text](../../../pic/AI/OpenAI/transformer3.png)

可以看出 Transformer 架构由 Encoder 和 Decoder 两个部分组成：其中 Encoder 和 Decoder 都是由 N=6 个相同的层堆叠而成。Multi-Head Attention 结构是 Transformer 架构的核心结构，其由多个 Self-Attention 组成的。Encoder block 只包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Self-Attention 中文翻译为自注意力机制，论文中叫作 Scale Dot Product Attention.
