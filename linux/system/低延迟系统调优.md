# 0x00. 导读

[red hat](https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf)

[configuration_suggestions](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-cpu-configuration_suggestions)

[kernel-parameters.txt](https://android.googlesource.com/kernel/common/+/bcmdhd-3.10/Documentation/kernel-parameters.txt)

使用工具 hiccups 测试延迟，[hiccups](https://github.com/rigtorp/hiccups)
```bash
# 测量 CPU 0 至 3 上的抖动
$ taskset -c 0-3 hiccups | column -t
```

[延迟可视化](https://colin-scott.github.io/personal_website/research/interactive_latency.html)

# 0x01. 简介

```
intel_idle.max_cstate=0 processor.max_cstate=0: 禁用CPU的深度睡眠状态，以提高系统的响应速度。

idle=poll: 禁用Linux的默认空闲状态处理程序，改为使用轮询方式。会阻止处理器进入 idle state；

pcie_aspm=performance pcie_port_pm=off: 禁用PCIe设备的省电模式，以提高性能。

mce=ignore_ce: 忽略内存错误检查，以提高性能。

ipmi_si.force_kipmi=0 nmi_watchdog=0 hpet=disabled: 禁用IPMI、NMI看门狗和HPET定时器，以提高性能。

noht nohz=on nohalt nosoftlockup: 禁用超线程、动态时钟调整、halt指令和软锁定，以提高性能。

intel_pstate=disable: 禁用Intel P-state调节器，以提高性能。

rcu_nocb_poll: 禁用RCU调节器中的回调函数，以提高性能。

transparent_hugepage=always: 启用透明大页面，以提高性能。

audit=0: 禁用审计子系统( audit subsyste )的内核组件，在重负载下，该子系统的 CPU 利用率经测量约为 1%

irqaffinity=0,1: 将默认 IRQ 掩码设置为内核 0 和 1。

skew_tick=1: 调整时钟偏差，以提高性能。

selinux=0: 禁用SELinux安全机制，以提高性能。

clocksource=tsc tsc=reliable: 使用TSC时钟源，以提高性能。

ipv6.disable=1: 禁用IPv6协议，以提高性能。

iommu=off intel_iommu=off: 禁用IOMMU，以提高性能。

noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off tsx=on tsx_async_abort=off mitigations=off: 禁用各种CPU安全漏洞的修复措施，
以提高性能。

hugepagesz=2m hugepages=128: 启用大页面，以提高性能。

isolcpus=3-17 rcu_nocbs=3-17 nohz_full=3-17: 将CPU3到CPU17独立出来，不参与系统调度，以提高性能。
```

```
crashkernel=auto：指定内核映像中的crashkernel大小，用于系统崩溃时的内核转储。

rd.lvm.lv=centos/root、rd.md.uuid=fe6f1825:02a14268:2d74c18d:942c9e17、rd.md.uuid=0392e0ba:3db7bb4a:25b1a05f:0d6e0927、rd.lvm.lv=centos/swap：指定根文件系统和交换分区的位置。

rhgb quiet：启用图形启动和禁用内核消息。
```

# 0x02. 操作

硬件层面一些注意的问题：
- 提高 cache 命中率，L1 L2 L3, LLC--Last Level Cache
- 提高分支预测准确率
- Multi-Core

    接收 IRQ 的 CPU 可能不是响应者休眠的 CPU，在这种情况下，前者必须向后者发送重新调度请求，以便它恢复响应者。这通常是通过处理器间中断完成的，也就是IPI，IPI的发送和处理进一步增加了延迟。

    此外，多核总线共享、LLC共享，NUMA架构远端内存访问等，均会导致访问延迟不确定。
- DMA

    DMA突发传输不确定占用总线带宽，影响程序执行；
- SMI

    系统管理中断(System Management Interrupts)或 SMI 是最高优先级的特殊中断，导致 x86 CPU 进入系统管理模式 SMM(System Management Mode)，这是实模式的一种变体，用于执行 BIOS 实现的某些处理程序，在这种模式下，包括操作系统在内的所有正常执行都暂停。SMI 不通过中断控制器，它们在指令之间被 CPU 逻辑检测到，并从那里无条件地调度。这给实时系统带来了关键问题：

    SMI 可以在任何时候在不确定的时间内抢占实时代码，并且不能被内核软件屏蔽或抢占。实际上，内核软件甚至不知道正在进行的 SMI 请求。
    转换到 SMM 上下文或从 SMM 上下文转换需要 CPU 保存/恢复其大部分寄存器文件，切换到不同的 CPU 模式。对于多核系统，BIOS 甚至可以等待所有 CPU 内核进入 SMM，然后再序列化待处理的 SMI 请求的执行。这是意外延迟的另一个来源。
    SMI 调用的 SMM 处理程序是在 BIOS 中实现的，因此它们的实现对我们来说是不透明的。我们可能只是观察到其中一些引起延迟变大的点（例如，很常见看到USB相关SMI的300微秒延迟）。

    **因此，在购买此类硬件之前，强烈建议您提前向供应商详细了解其关于SMI问题的解决方案，或者自行测试硬件在处理SMI方面的性能表现，以免影响实际应用中实时性的要求。**

- 硬件采购

    除处理器外，内存方面，使用双通道内存，尽可能高的内存频率。

    散热当面，针对处理器工作负载设计良好的散热结构, 否则芯片保护会强制降频，频率调整CPU会停顿几十上百us。

软件层面一些注意的问题：
- 操作系统：
    - 调度算法，同步机制，优先级倒置，信号量类型，通信语义和中断处理等。
    - 异常处理方式：中断线程化、中断上下部
    - 内存管理
    - 内存访问时间不确定性：MMU、cache、NUMA
    - 惰性分配策略：Pagefault、CoW(Copy of Write)
    - 内存分配算法时间不确定性：快速分配路径 vs 慢速分配路径（内存合并和回收）、OOM (Out Of Memory) killer、swap
    - RT调度策略：FIFO、RR...
    - 实时驱动

- 资源的分配隔离：分配CPU专门对实时任务服务、将多余中断隔离到非实时任务CPU上，分配CPU专门对实时任务服务可使L1 、L2 Cache只为实时任务服务。

## 2.1 绑核

通过 taskset 或 sched_setaffinity 
```bash
$ taskset -c 0-3 hiccups
```

## 2.2 CPU 隔离

例如： isolcpus=0,3 表示隔离 CPU0 3

以上只是 linux 不会调度普通任务到 CPU 0 3 上运行,这是基础，此时还需要设置绑核，让程序只在这些核上运行。

任务通过函数设置任务只在 CPU0 和 3 上调度，隔离后的CPU的L1、L2缓存命中率相应的也会得到提高。
```c
cpu_set_t cpuset;
CPU_ZERO(&cpuset);
CPU_SET(3, &cpuset);//将线程限制在指定的cpu3上运行
const auto rc =
    pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
if (rc != 0) {
    printf("error Setting affinity to {}", 3);
    return;
}
```

当使用 isolcpus 时，内核仍然会在隔离的内核上创建多个内核线程。其中一些内核线程可以移动到非隔离核心。
```bash
# 尝试将所有内核线程移至核心 0：
$ pgrep -P 2 | xargs -i taskset -p -c 0 {}

# 使用 tuna 将所有内核线程从核心 1-7 移开
$ tuna --cpus=1-7 --isolate
```

使用 tuna 命令进行验证以显示所有线程的 CPU 关联性：
```bash
$ tuna -P
```

内核工作队列需要从隔离内核中移出。要将所有工作队列移至核心 0 (cpumask 0x1)：
```bash
$ find /sys/devices/virtual/workqueue -name cpumask  -exec sh -c 'echo 1 > {}' ';'
```
通过列出当前工作队列关联性进行验证：
```bash
find /sys/devices/virtual/workqueue -name cpumask -print -exec cat '{}' ';'
```

最后通过检查每个核心发生了多少线程上下文切换来验证核心是否已成功隔离：
```bash
$ perf stat -e 'sched:sched_switch' -a -A --timeout 10000
```
隔离的核心应该显示非常低的上下文切换计数。

### 2.2.1 Full Dynamic Tick

`grep CONFIG_NO_HZ /boot/config-$(uname -r)`

将 CPU0、CPU3 作为隔离后，我们已将 linux 任务从这两个 cpu 上剔除， CPU 上 Tick 也就没啥用了，避免多余的 Tick 中断影响实时任务的运行，需要将这两个 cpu 配置为 Full Dynamic Tick 模式，即关闭 tick。通过添加 linux 内核参数 nohz_full=[cpu列表] 配置。

nohz_full=[cpu列表] 在使用 CONFIG_NO_HZ_FULL = y 构建的内核中才生效。

需要注意的是，只有当该核心上只调度了一个可运行线程时，计时器才会在该核心上禁用。您可以在 /proc/sched_debug 中查看每个核心的可运行线程数(see runnable tasks)。

**注意：boot CPU(通常是0号CPU)会无条件的从列表中剔除。**

最后，您可以通过检查 /proc/interrupts 或使用 perf 监视定时器中断来验证定时器中断频率是否降低：
```bash
$ perf stat -e 'irq_vectors:local_timer_entry' -a -A --timeout 30000

 Performance counter stats for 'system wide':

CPU0                   31,204      irq_vectors:local_timer_entry
CPU1                    3,771      irq_vectors:local_timer_entry
CPU2                        3      irq_vectors:local_timer_entry
CPU3                        4      irq_vectors:local_timer_entry

      30.001671482 seconds time elapsed
```
在上面的示例中，内核 2 和 3 的定时器中断频率降低。期望 isolcpus + nohz_full 内核每隔一秒左右显示一次计时器中断。不幸的是，计时器滴答声无法完全消除。

### 2.2.2 Offload RCU callback

从引导选择的 CPU 上卸载 RCU 回调处理,使用内核线程 “rcuox / N”代替，通过linux内核参数 rcu_nocbs=[cpu列表] 指定的CPU列表设置。这对于HPC和实时工作负载很有用，这样可以减少卸载RCU的CPU上操作系统抖动。

```
rcuox / N, N 表示 CPU 编号。
x 可以是
    'b', 代表的是 RCU-bh 的 b
    'p', 代表的是 RCU-preempt 的 p
    's', 代表的是 RCU-sched 的 s
```

rcu_nocbs=[cpu列表] 在使用 CONFIG_RCU_NOCB_CPU=y 构建的内核中才生效。

除此之外需要设置RCU内核线程 rcuc/n 和 rcub/n 线程的 SCHED_FIFO 优先级值 RCU_KTHREAD_PRIO, RCU_KTHREAD_PRIO 设置为高于最低优先级线程的优先级，也就是说至少要使该优先级低于xenomai实时应用的优先级，避免xenomai实时应用迁移到linux后，由于优先级低于RCU_KTHREAD的优先级而实时性受到影响，如下配置RCU_KTHREAD_PRIO=0。

也手动将 rcu 线程移到非延迟敏感的内核中
```bash
# 这是移动到 core 0
for i in `pgrep rcu[^c]` ; do taskset -pc 0 $i ; done
```

### 2.2.3 中断隔离

列出所有 IRQ 的 CPU 关联性：
```bash
$ find /proc/irq/ -name smp_affinity_list -print -exec cat '{}' ';'
```

多核情况下，通过内核参数 irqaffinity==[cpu列表]，设置linux设备中断的亲和性，设置后，默认由这些cpu核来处理中断。避免了非实时linux中断影响 cpu2、cpu3 上的实时应用，将linux中断指定到cpu0、cpu1处理。

最后通过监视 /proc/interrupts 验证隔离内核没有接收中断：
```bash
$ watch cat /proc/interrupt
```

### 2.2.4 禁用irqbanlance

linux irqbalance 用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。简单来说 irqbalance 会将硬件中断分配到各个CPU核心上处理。

- 处于 Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。
- 处于 Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗。

禁用irqbanlance，避免不相干中断发生在RT任务核。x86平台还可添加参数 acpi_irq_nobalance 禁用ACPI irqbalance.

### 2.2.5 nmi_watchdog[x86]

NMI watchdog 是 Linux 的开发者为了 debugging 而添加的特性，但也能用来检测和恢复 Linux kernel hang ，现代多核x86体系都能支持 NMI watchdog 。

NMI（Non Maskable Interrupt）即不可屏蔽中断，之所以要使用NMI，是因为NMI watchdog 的监视目标是整个内核，而内核可能发生在关中断同时陷入死循环的错误，此时只有NMI能拯救它。

Linux 中有两种 NMI watchdog，分别是 I/O APIC watchdog(nmi_watchdog=1) 和 Local APIC watchdog(nmi_watchdog=2) 。它们的触发机制不同，但触发NMI之后的操作是几乎一样的。一旦开启了 I/O APIC watchdog（nmi_watchdog=1），那么每个CPU对应的Local APIC的LINT0线都关联到NMI，这样每个CPU将周期性地接到NMI，接到中断的CPU立即处理NMI，用来悄悄监视系统的运行。如果系统正常，它啥事都不做，仅仅是更改 一些时间计数；如果系统不正常（默认5秒没有任何普通外部中断），那它就闲不住了，会立马跳出来，中止程序的运行。

避免周期中断的 NMI watchdog 影响实时性需要关闭 NMI watchdog，传递内核参数 nmi_watchdog=0.
禁用超线程，传递内核参数 noht.

### 2.2.6 nosoftlockup 

linux 内核参数，禁用 soft-lockup 检测器。进程在 CPU 上执行的时间超过 softlockup 阈值（默认 120 秒）时禁用回溯跟踪日志记录。

## 2.3 屏蔽硬件中断

避免 IRQ 向某个 core 发中断可以通过改写 `/proc/irq/*/smp_affinity` 来实现。

## 2.4 屏蔽软件中断

### 2.4.1 work queue

改写 `/sys/devices/virtual/workqueue/*/cpumask` 。
屏蔽了 workqueue 后，虽然 kwoker 线程还在，但不会影响 core .

### 2.4.2 LOC(local timer interrupt)

也就是 `/proc/interrupts` 中的 `LOC` 。
为了减少收到的 LOC 频率，我们还需要使用 `adaptive-ticks` 模式，这可以通过 `nohz_full` 和 `rcu_nocbs` 启动选项实现。并且还需要设置 `rcu_nocbs=` 。

adaptive-ticks 的效果是：如果 core 上的 running task 只有一个，系统向其发送 LOC 的频率会降低成每秒一次，内核文档解释了不能完全屏蔽LOC的原因：
>"Some process-handling operations still require the occasional scheduling-clock tick. These operations include calculating CPU load, maintaining sched average, computing CFS entity vruntime, computing avenrun, and carrying out load balancing. They are currently accommodated by scheduling-clock tick every second or so. On-going work will eliminate the need even for these infrequent scheduling-clock ticks."。

## 2.5 电源管理与调频

为了省电，让操作系统随着工作量不同，动态调节CPU频率和电压。但是调频会导致CPU停顿（CPU停顿时间10us~500us不等），运行速度降低导致延迟增加，严重影响实时性能。

除了调频以外，另一个严重影响实时性的是，系统进入更深层次的省电睡眠状态，这时的唤醒延迟长达几十毫秒。

检查涡轮增压是否启用:
```bash
# 0 表示启动
$ cat /sys/devices/system/cpu/intel_pstate/no_turbo
0
```
或者，可以使用 cpupower 来检查 :
```bash
$ cpupower frequency-info
```

```bash
# /usr/lib/tuned/network-latency/tuned.conf 里面也有很多配置内容可以参考
$ tuned-adm profile latency-performance
```

## 2.6 关闭超线程

人们对CPU的性能的追求是无止境的，在CPU性能不断优化提高过程中，对于单一流水线，最佳情况下，IPC 也只能到 1。无论做了哪些流水线层面的优化，即使做到了指令执行层面的乱序执行，CPU 仍然只能在一个时钟周期里面取一条指令。

为使IPC>1，诞生了多发射（Mulitple Issue）和超标量（Superscalar）技术，伴随的是每个CPU流水线上各种运算单元的增加。但是当处理器在运行一个线程，执行指令代码时，一方面很多时候处理器并不会使用到全部的计算能力，另一方面由于CPU在代码层面运行前后依赖关系的指令，会遇到各种冒险问题，这样CPU部分计算能力就会处于空闲状态。

为了进一步“压榨”处理器，那就找没有依赖关系的指令来运行好，即另一个程序。一个核可以分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程，又称同时多线程（Simultaneous Multi-Threading，简称 SMT）。

由于超线程技术通过双份的 PC 寄存器、指令寄存器、条件码寄存器，在逻辑层面伪装为2个CPU，但指令译码器和ALU是公用的，这就造成实时任务运行时在CPU执行层面的不确定性，造成非实时线程与实时线程在同一物理核上对CPU执行单元的竞争，影响实时任务实时性。

有多种方法可以禁用 SMT/HT：

- 在系统的 UEFI 或 BIOS 固件设置中。这是我推荐的方法。
- 在启动时使用内核命令行参数 nosmt 。
- 在运行时使用 SMT 控制：  
    `echo off > /sys/devices/system/cpu/smt/control`

要验证 SMT/HT 是否已禁用，以下命令的输出应为 0：
```bash
$ cat /sys/devices/system/cpu/smt/active
```

## 2.7 CPU特性[x86]

intel处理器相关内核参数：

- nosmap
- nohalt。告诉内核在空闲时,不要使用省电功能 PAL_HALT_LIGHT。 这增加了功耗。但它减少了中断唤醒延迟，这可以提高某些环境下的性能，例如联网服务器或实时系统。
- mce=ignore_ce,忽略machine checkerrors (MCE).
- idle=poll,不要使用HLT在空闲循环中进行节电，而是轮询以重新安排事件。 这将使CPU消耗更多的功率，但对于在多处理器基准测试中获得稍微更好的性能可能很有用。 它还使使用性能计数器的某些性能分析更加准确。
- clocksource=tsc tsc=reliable,指定tsc作为系统clocksource.
- intel_idle.max_cstate=0 禁用intel_idle并回退到acpi_idle.
- processor.max_cstate=0 intel.max_cstate=0 processor_idle.max_cstate=0 限制睡眠状态c-state。
- hpet=disable clocksource=tsc tsc=reliable 系统clocksource相关配置。

## 2.8 内核构建配置

`/boot/config-$(uname -r)`

系统构建时，除以上提到的配置外（CONFIG_NO_HZ_FULL = y、CONFIG_RCU_NOCB_CPU=y、RCU_KTHREAD_PRIO=0），其他实时性相关配置如下：

CONFIG_MIGRATION=n、CONFIG_MCORE2=y[x86]、CONFIG_PREEMPT=y、ACPI_PROCESSOR =n[x86]、CONFIG_CPU_FREQ =n、CONFIG_CPU_IDLE =n；

## 2.9 

默认情况下，虚拟内存子系统每 1 秒运行一次每个核心统计信息更新任务。您可以通过将 vm.stat_interval 设置为更高的值（例如 120 秒）来缩短此间隔：
```bash
$ sysctl vm.stat_interval=120
```

## 2.10 禁用 swap

```bash
$ swapoff -a
```

## 2.11 禁用透明大页

transparent huge page (THP), 透明大页 (THP) 支持允许内核自动将常规内存页提升为大页。大页面可以降低 TLB 压力，但当页面提升为大页面以及触发内存压缩时，THP 支持会引入延迟峰值。

通过提供内核命令行参数 `transparent_hugepage=never` 或运行以下命令来禁用透明大页支持：
```bash
$ echo never > /sys/kernel/mm/transparent_hugepage/enabled
```

```bash
# 检查
$cat /sys/kernel/mm/transparent_hugepage/enabled
[always] madvise never  # 这样是启用
always madvise [never]  # 这样是禁用
```

## 2.12 禁用 NUMA 内存平衡

在 NUMA 计算机上，如果 CPU 访问远程内存，则会降低性能。启用此功能后，内核会通过定期取消映射页面并捕获页面错误来采样哪个任务线程正在访问内存。在发生页面错误时，确定是否应将正在访问的数据迁移到本地存储节点。页面的取消映射和陷阱错误会导致额外的开销，理想情况下，这些开销会被改进的内存局部性所抵消，但不保证其普遍性。如果目标工作负载已绑定到 NUMA 节点，则应禁用此功能。

使用以下命令禁用自动 NUMA 内存平衡：
```bash
$ echo 0 > /proc/sys/kernel/numa_balancing
```

## 2.13 Disable mitigations for CPU vulnerabilities(禁用 CPU 漏洞缓解措施)

将 mitigations=off 添加到内核命令行以禁用所有缓解措施。

## 2.14 避免 TLB shootdowns

TLB(Translation Lookaside Buffer) shootdowns:  在 SMP 系统下，如果某个页表被多个CPU共享，当某个CPU修改了该页表的地址映射，则需要通知其他正在用该页表的CPU去更新自己的TLB，否则其他CPU在进行地址转换时就不能得到正确的地址映射。怎么通知？在X86上是通过发送IPI中断来实现的，即给其他CPU发一个中断，触发相应的vector的handler去flush TLB。然后修改页表的CPU会一直loop（OS设置了全局变量，来表示对应CPU有没有flush完TLB），直到所有其他CPU完成flush TLB。

可以在 /proc/interrupts 中查看每个 CPU 核心的 TLB 击落次数：
```bash
$ egrep 'TLB|CPU' /proc/interrupts
            CPU0       CPU1       CPU2       CPU3
 TLB:   16642971   16737647   16870842   16350398   TLB shootdowns
```

## 2.15 skew_tick

调整时钟偏差，以提高性能。

Using the skew_tick=1 boot parameter reduces contention(竞争) on these kernel locks. The parameter ensures that the ticks per CPU do not occur simultaneously by making their start times 'skewed'(歪斜，倾斜). Skewing the start times of the per-CPU timer ticks decreases the potential for lock conflicts, reducing system jitter for interrupt response time

```
skew_tick=1
```

## 2.16 SELinux

```bash
# #关闭SELinux立即生效，重启服务器后失效。
$ setenforce 0

#修改配置文件需要重启服务器配置才会生效.
$ sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```


# 0x03. 测试

## 3.1

观察 `/proc/interrupts` 变化。

## 3.2 

```c++
#include <iostream>

uint64_t now() {
   return __builtin_ia32_rdtsc();
}

int main() {
  uint64_t last = now();
  while (true) {
    uint64_t cur = now();
    uint64_t diff = cur - last;
    if (diff > 300) {
      std::cout << "latency: " << diff << " cycles" << std::endl;
      cur = now();
    }
    last = cur;
  }
  return 0;
}
```

通过 taskset 绑定一个核运行程序，每进入一次内核会打印一条信息。

# 0x04. 网络

[优化参数](../net/优化参数.md)

## 4.2 

```
LD_LIBRARY_PATH=/home/alfred/skyroad onload --profile=latency-best ./SkyRoadTD 12 sr_vendor.aeg 1>$logName 2>&1 &

1.systemctl set-default multi-user.target
2.tuned-adm profile network-latency
    tuned-adm active
3.x86_energy_perf_policy performance
    x86_energy_perf_policy -r
    cpupower frequency-set --governor performance
    cpupower frequency-info
    turbostat -S
```