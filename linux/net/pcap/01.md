# 0x00. 导读

# 0x01. 简介

# 0x02. 

Libpcap 是 Packet Capture Libray 的英文缩写，即数据包捕获函数库。该库提供的 C 函数接口用于捕捉经过指定网络接口的数据包，该接口应该是被设为 **混杂模式(promiscuous)**。这个在原始套接子中有提到。

主要功能：

- 数据包捕获：捕获流经网卡的原始数据包
- 自定义数据包发送：构造任何格式的原始数据包
- 流量采集与统计：采集网络中的流量信息
- 规则过滤：提供自带规则过滤功能，按需要选择过滤规则

## 2.1 安装

```bash
$ sudo yum install libpcap-dev
```

# 0x03. 原理

正常情况下接收的网络数据包首先会进到网卡，网卡接收到数据包发送中断通知 cpu, cpu 根据中断号找中断向量和对应驱动的中断处理函数，中断处理函数对网卡过来的数据做一层封装，输出成内核网络包相关数据结构的类型 (sk_buff/msbuf), 之后发送到数据链路层做报文的合法性检查，然后发送给更上层的 tcp/ip 协议栈对数据包做报文解析，最终应用层程序通过文件描述符 (inode) 将内核队列中的网络数据通过 socket 通道拷贝到用户态进行处理。

libpcap 的工作原理是：当一个数据包到达网卡时，通过内核提供的旁路机制拷贝一份数据包到 bpf(BSD Packet Filter) 过滤器，其实现方式是通过创建内核 socket ，接收来自数据链路层驱动的网络数据包，然后使用 bpf 自身的 the network tap 组件接收来自 socket 的数据包并发送给 the bpf filter 过滤组件； bpf 根据已经定义好的包过滤规则对数据包进行过滤，匹配成功的放到内核缓冲区，并等待用户态调系统调用去取（read）；匹配失败的数据包则直接 drop。

一个包的捕捉分为三个主要部分，包括面向底层包捕获、面向中间层的数据包过滤和面向应用层的用户接口。这与 Linux 操作系统对数据包的处理流程是相同的（网卡->网卡驱动->数据链路层->IP层->传输层->应用程序）。包捕获机制是在数据链路层增加一个旁路处理（并不干扰系统自身的网络协议栈的处理），对发送和接收的数据包通过 Linux 内核做过滤和缓冲处理，最后直接传递给上层应用程序。

Libpcap的抓包流程：

1. 查找网络设备：目的是发现可用的网卡，实现的函数为 pcap_lookupdev() ，如果当前有多个网卡，函数就会返回一个网络设备名的指针列表。
2. 打开网络设备：利用上一步中的返回值，可以决定使用哪个网卡，通过函数 pcap_open_live() 打开网卡，返回用于捕捉网络数据包的秒数字。
3. 获得网络参数：这里是利用函数 pcap_lookupnet()，可以获得指定网络设备的IP地址和子网掩码。
4. 编译过滤策略：Lipcap的主要功能就是提供数据包的过滤，函数pcap_compile()来实现。
5. 设置过滤器：在上一步的基础上利用pcap_setfilter()函数来设置。
6. 利用回调函数，捕获数据包：函数pcap_loop()和pcap_dispatch()来抓去数据包，也可以利用函数pcap_next()和pcap_next_ex()来完成同样的工作。
7. 关闭网络设备：pcap_close()函数关系设备，释放资源。

## 3.1 过滤数据包

libpcap利用BPF来过滤数据包。

过滤数据包需要完成3件事：
1. 构造一个过滤表达式。

    Lipcap 已经把 BPF 语言封装成为了更高级更容易的语法了。
    ```
    src host 127.0.0.1
    //选择只接受某个IP地址的数据包

    dst port 8000
    //选择只接受TCP/UDP的目的端口是80的数据包

    not tcp
    //不接受TCP数据包

    tcp[13]==0x02 and (dst port ** or dst port **)
    //只接受SYN标志位置（TCP首部开始的第13个字节）且目标端口号是22或23的数据包

    icmp[icmptype]==icmp-echoreply or icmp[icmptype]==icmp-echo
    //只接受icmp的ping请求和ping响应的数据包

    ehter dst 00:00:00:00:00:00
    //只接受以太网MAC地址为00：00：00：00：00：00的数据包

    ip[8]==5
    //只接受ip的ttl=5的数据包（ip首位第八的字节为ttl）
    ```

2. 编译这个表达式

    使用 pcap_compile() 函数来编译
    ```c
    int pcap_compile(pcap_t * p, struct bpf_program * fp, char * str, int optimize, bpf_u_int32 netmask)
    //fp：这是一个传出参数，存放编译后的 bpf
    //str：过滤表达式
    //optimize：是否需要优化过滤表达式
    //metmask：简单设置为0即可
    ```

3. 应用这个过滤器

    通过函数 pcap_setfilter() 来设置这个规则
    ```c
    int pcap_setfilter(pcap_t * p,  struct bpf_program * fp)
    //参数 fp 就是 pcap_compile() 的第二个参数，存放编译后的 bpf
    ```

可以在抓包前，也就是 pcap_next() 或 pcap_loop 之前，加入下面的代码：
```c
//design filter  
struct bpf_program filter;  
pcap_compile(device, &filter, "dst port 80", 1, 0);  // 只接受 80 端口的 TCP/UDP 数据包
pcap_setfilter(device, &filter); 
```

## 3.2 性能

libpcap 的性能问题在于：

1. 内核通过旁路机制 copy 的流量会走 bpf 过滤，加重了对内核的负担，不过由于libpcap在bpf这块的规则并不复杂，所以影响不是非常大

2. 用户态通过系统调用从内核缓存中取数据，而走系统调用在用户/内核间来回切换效率是非常低的，因为系统调用要做一系列操作，包括上下文环境保存、切换内核栈以及某些系统调用额外的一些安全性检查等等，导致的结果就是频繁的系统调用不仅慢而且耗 cpu，这个是 **libpcap 性能问题的根源**:

    - 数据包到达网卡设备。
    - 网卡设备依据配置进行DMA操作。（ 「第1次拷贝」 ：网卡寄存器->内核为网卡分配的缓冲区ring buffer）
    网卡发送中断，唤醒处理器。
    - 驱动软件从ring buffer中读取，填充内核skbuff结构（ 「第2次拷贝」 ：内核网卡缓冲区ring buffer->内核专用数据结构skbuff）
    - 接着调用netif_receive_skb函数：
    - 
        - 5.1 如果有抓包程序，由网络分接口进入BPF过滤器，将规则匹配的报文拷贝到系统内核缓存 （ 「第3次拷贝」 ）。BPF为每一个要求服务的抓包程序关联一个filter和两个buffer。BPF分配buffer 且通常情况下它的额度是4KB the store buffer 被使用来接收来自适配器的数据；the hold buffer被使用来拷贝包到应用程序。
        - 5.2 处理数据链路层的桥接功能；
        - 5.3 根据skb->protocol字段确定上层协议并提交给网络层处理，进入网络协议栈，进行高层处理。
    - libpcap绕过了Linux内核收包流程中协议栈部分的处理，使得用户空间API可以直接调用套接字PF_PACKET从链路层驱动程序中获得数据报文的拷贝，将其从内核缓冲区拷贝至用户空间缓冲区（ 「第4次拷贝」 ）

3. libpcap 本身未与进程关联，用户态拿到的还是只是五元组，要与进程关联还是只能遍历 proc(/proc/net/tcp(6))，这块的cpu消耗也不低，甚至在一些大流量环境下占用会非常高；但是比不调度的纯proc遍历要稍好，毕竟是走网络包回调的。

### 3.2.1 PF_PACKET

`pcap_activate`

新版本（Packet MMAP support was integrated into libpcap around the time of version 1.3.0; TPACKET_V3 support was added in version 1.5.0）的 libpcap 基本都采用 packet_mmap 机制。PACKET_MMAP 通过 mmap，减少一次内存拷贝（ 「第4次拷贝没有了」 ），减少了频繁的系统调用，大大提高了报文捕获的效率。([packet_mmap.txt](https://docs.kernel.org/networking/packet_mmap.html))

TPACKET_Vx 各个版本的区别是：
- TPACKET_V1 : 这是缺省的环形缓冲区版本
- TPACKET_V2 : 相比 V1 的改进有以下几点
    - 32 位的用户空间环形缓冲区可以基于 64 位内核工作;
    - 时间戳的精度从 ms 提升到 ns;
    - 支持携带 VLAN 信息 (这意味着通过 V1 接收到的 vlan 包将会丢失 vlan 信息)；
- TPACKET_V3 : 相比 V2 的改进有以下几点
    - 内存块可以配置成可变帧长 (V1、V2 的帧长都是 tpacket_req.tp_frame_size 固定值);
    - read/poll 基于 block-level(V1、V2 基于 packet_level);
    - 开始支持 poll 超时参数；
    - 新增了用户可配置选项：tp_retire_blk_tov 等；
    - RX Hash 数据可以被用户使用；

极端的还有 PF_RING 技术和 DPDK 。

## 3.3 混杂模式

在网卡缺省的工作模式下，只能收到广播的数据包和目的地址是自己的数据包。所以在进行数据包捕获时，我们首先要将网卡设置为混杂模式，这样就能捕获到流经该网卡的所有数据包。值得注意的是，捕获到的仅仅是数据包的一份拷贝，不影响数据包的正常传输。

正常情况下当网络数据包到达网卡时，它常规的传输路径是依次经过网卡、设备驱动器、数据链路层、IP层、传输层、最后到达用户层。Libcap 包捕获机制是在数据链路层增加一个旁路处理。当一个数据包到达网络接口时，Libpcap 首先利用已经创建的 Socket 从链路层驱动程序获得该数据包的拷贝，再通过 Tap 函数将数据包发送给 BPF 过滤器。 BPF 过滤器收到数据包后，根据用户已经定义好的过滤规则对数据包进行逐一的匹配，符合过滤规则的数据包就是我们需要的，将它放入内核缓冲器，并传递给用户层缓冲器，等待应用程序对其进行处理。不符合过滤规则的数据包就被丢弃。如果没有设定过滤规则，所有的数据包都将被放入内核缓冲器。
